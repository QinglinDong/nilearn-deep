{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 load the ADHD200 data\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uga_qinglin/Documents/nilearn-deep/nilearn/datasets/func.py:503: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First functional nifti image (4D) is at: /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.fit] Loading data from [/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz, /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.g\n",
      "[MultiNiftiMasker.fit] Computing mask\n",
      "[MultiNiftiMasker.transform] Resampling mask\n",
      "[CanICA] Loading data\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010128/0010128_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0021019/0021019_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023008/0023008_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023012/0023012_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027011/0027011_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027018/0027018_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027034/0027034_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027037/0027037_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1019436/1019436_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1206380/1206380_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1418396/1418396_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1517058/1517058_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1552181/1552181_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1562298/1562298_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1679142/1679142_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2014113/2014113_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2497695/2497695_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2950754/2950754_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3007585/3007585_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3154996/3154996_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3205761/3205761_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3520880/3520880_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3624598/3624598_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3699991/3699991_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3884955/3884955_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3902469/3902469_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3994098/3994098_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/4016887/4016887_rest_tshift_RPI_voreg_mni.nii.gz\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "#dir='/raid/nilearn_data'\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=30,data_dir='/home/share/TmpData/Qinglin/nilearn_data/')\n",
    "func_filenames = adhd_dataset.func  # list of 4D nifti files for each subject\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First functional nifti image (4D) is at: %s' %\n",
    "      func_filenames[0])  # 4D data\n",
    "\n",
    "from nilearn.decomposition import CanICA\n",
    "\n",
    "canica = CanICA(n_components=20, smoothing_fwhm=6.,\n",
    "                memory=\"nilearn_cache\", memory_level=2,\n",
    "                threshold=3., verbose=10, random_state=0)\n",
    "data=canica.prepare_data(func_filenames)\n",
    "original_dim=data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 apply CanICA on the data\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(original_dim,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "\n",
    "encoded = Dense(256, activation='tanh',\n",
    "                activity_regularizer=regularizers.l1(2*10e-5))(input_img)\n",
    "encoded = Dense(128, activation='tanh',\n",
    "                activity_regularizer=regularizers.l1(2*10e-5))(encoded)\n",
    "encoded = Dense(32, activation='tanh',\n",
    "                activity_regularizer=regularizers.l1(2*10e-5))(encoded)\n",
    "\n",
    "decoded = Dense(128, activation='tanh')(encoded)\n",
    "decoded = Dense(256, activation='tanh')(decoded)\n",
    "decoded = Dense(original_dim, activation='tanh')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "#SVG(model_to_dot(autoencoder).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nbviewer.jupyter.org/github/shikanon/MyPresentations/blob/master/DeepLearning/LearnOfDeepLearning.ipynb#1.-softmax\n",
    "\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "import tensorflow as tf\n",
    "\n",
    "#构建一个记录的loss的回调函数\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "# 构建一个自定义的TensorBoard类，专门用来记录batch中的数据变化\n",
    "class BatchTensorBoard(TensorBoard):\n",
    "    def __init__(self,log_dir='./logs',\n",
    "                 histogram_freq=0,\n",
    "                 write_graph=True,\n",
    "                 write_images=False):\n",
    "        super(BatchTensorBoard, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.histogram_freq = histogram_freq\n",
    "        self.merged = None\n",
    "        self.write_graph = write_graph\n",
    "        self.write_images = write_images\n",
    "        self.batch = 0\n",
    "        self.batch_queue = set()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self,batch,logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        self.batch = self.batch + 1\n",
    "        \n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = float(value)\n",
    "            summary_value.tag = \"batch_\" + name\n",
    "            if (name,self.batch) in self.batch_queue:\n",
    "                continue\n",
    "            self.writer.add_summary(summary, self.batch)\n",
    "            self.batch_queue.add((name,self.batch))\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='/home/uga_qinglin/Documents/nilearn-extenstion/experiments/tensorflow/log/softmax/epoch')\n",
    "my_tensorboard = BatchTensorBoard(log_dir='/home/uga_qinglin/Documents/nilearn-extenstion/experiments/tensorflow/log/softmax/batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [256,128] and type float\n\t [[{{node training/Adadelta/zeros_14}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256,128] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training/Adadelta/zeros_14', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-59c0e48d7c28>\", line 5, in <module>\n    shuffle=True)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1010, in fit\n    self._make_train_function()\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 509, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 398, in get_updates\n    delta_accumulators = [K.zeros(shape) for shape in shapes]\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 398, in <listcomp>\n    delta_accumulators = [K.zeros(shape) for shape in shapes]\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 702, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1561, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2879, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [256,128] and type float\n\t [[{{node training/Adadelta/zeros_14}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256,128] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [256,128] and type float\n\t [[{{node training/Adadelta/zeros_14}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256,128] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-59c0e48d7c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [256,128] and type float\n\t [[{{node training/Adadelta/zeros_14}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256,128] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training/Adadelta/zeros_14', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-59c0e48d7c28>\", line 5, in <module>\n    shuffle=True)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1010, in fit\n    self._make_train_function()\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 509, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 398, in get_updates\n    delta_accumulators = [K.zeros(shape) for shape in shapes]\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 398, in <listcomp>\n    delta_accumulators = [K.zeros(shape) for shape in shapes]\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 702, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1561, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2879, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [256,128] and type float\n\t [[{{node training/Adadelta/zeros_14}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256,128] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autoencoder.fit(data, data,\n",
    "                epochs=100,\n",
    "                batch_size=100,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=encoder.predict(data)\n",
    "\n",
    "#plot the activation: y\n",
    "import numpy as np\n",
    "t = range(0, y.shape[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(t, y[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "clf = Lasso(alpha=0.5)\n",
    "clf.fit(y,data)\n",
    "components_img=np.transpose(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=canica.masker_.mask_img_, standardize=True)\n",
    "masker.fit()\n",
    "\n",
    "import numpy as np\n",
    "components_img=canica.thresholding(components_img)\n",
    "components_img = masker.inverse_transform(components_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 plot each ICA component separately\n",
    "-----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"ortho\", title=\"IC %d\" % i,\n",
    "                  cut_coords=None, colorbar='bwr')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Extract regions from networks\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Region Extractor algorithm from regions module\n",
    "# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n",
    "# maps, less the threshold means that more intense non-voxels will be survived.\n",
    "from nilearn.regions import RegionExtractor\n",
    "\n",
    "extractor = RegionExtractor(components_img, threshold=0.5,\n",
    "                            thresholding_strategy='ratio_n_voxels',\n",
    "                            extractor='local_regions',\n",
    "                            standardize=True, min_region_size=5000)\n",
    "# Just call fit() to process for regions extraction\n",
    "extractor.fit()\n",
    "# Extracted regions are stored in regions_img_\n",
    "regions_extracted_img = extractor.regions_img_\n",
    "# Each region index is stored in index_\n",
    "regions_index = extractor.index_\n",
    "# Total number of regions extracted\n",
    "n_regions_extracted = regions_extracted_img.shape[-1]\n",
    "\n",
    "from nilearn.plotting import plot_prob_atlas\n",
    "# Visualization of region extraction results\n",
    "title = ('%d regions are extracted from %s components.'\n",
    "         '\\nEach separate color of region indicates extracted region'\n",
    "         % (n_regions_extracted, 'all'))\n",
    "plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n",
    "                         title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Region signals extraction\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to do subjects timeseries signals extraction and then estimating\n",
    "# correlation matrices on those signals.\n",
    "# To extract timeseries signals, we call transform() from RegionExtractor object\n",
    "# onto each subject functional data stored in func_filenames.\n",
    "# To estimate correlation matrices we import connectome utilities from nilearn\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "confounds = adhd_dataset.confounds\n",
    "phenotypic = adhd_dataset.phenotypic\n",
    "adhd_time_series = []\n",
    "all_time_series = []\n",
    "site_names = []\n",
    "adhd_labels = []  # 1 if ADHD, 0 if control\n",
    "# Initializing ConnectivityMeasure object with kind='correlation'\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "for filename, confound, phenotypic in zip(func_filenames, confounds,phenotypic):\n",
    "    time_series = extractor.transform(filename, confounds=confound)   \n",
    "    all_time_series.append(time_series)\n",
    "    is_adhd = phenotypic['adhd']\n",
    "    if is_adhd:\n",
    "        adhd_time_series.append(time_series)\n",
    "    site_names.append(phenotypic['site'])\n",
    "    adhd_labels.append(is_adhd)        \n",
    "print('Data has {0} ADHD subjects.'.format(len(adhd_time_series)))        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3  Correlation coefficients\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix plotting from Nilearn: nilearn.plotting.plot_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_matrices(matrices, matrix_kind):\n",
    "    n_matrices = len(matrices)\n",
    "    fig = plt.figure(figsize=(n_matrices * 4, 4))\n",
    "    for n_subject, matrix in enumerate(matrices):\n",
    "        plt.subplot(1, n_matrices, n_subject + 1)\n",
    "        matrix = matrix.copy()  # avoid side effects\n",
    "        # Set diagonal to zero, for better visualization\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        vmax = np.max(np.abs(matrix))\n",
    "        title = '{0}, subject {1}'.format(matrix_kind, n_subject)\n",
    "        plotting.plot_matrix(matrix, vmin=-vmax, vmax=vmax, cmap='RdBu_r',\n",
    "                             title=title, figure=fig, colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrices = correlation_measure.fit_transform(adhd_time_series)\n",
    "        \n",
    "from nilearn import plotting\n",
    "plot_matrices(correlation_matrices[:4], 'correlation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 group-sparse precision matrices\n",
    "------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import GroupSparseCovarianceCV\n",
    "gsc = GroupSparseCovarianceCV(verbose=2)\n",
    "gsc.fit(adhd_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrices_2(cov, prec, title, labels=None):\n",
    "    \"\"\"Plot covariance and precision matrices, for a given processing. \"\"\"\n",
    "\n",
    "    prec = prec.copy()  # avoid side effects\n",
    "\n",
    "    # Put zeros on the diagonal, for graph clarity.\n",
    "    size = prec.shape[0]\n",
    "    prec[list(range(size)), list(range(size))] = 0\n",
    "    span = max(abs(prec.min()), abs(prec.max()))\n",
    "\n",
    "    # Display covariance matrix\n",
    "    plotting.plot_matrix(cov, cmap=plotting.cm.bwr,\n",
    "                         vmin=-1, vmax=1, title=\"%s / covariance\" % title,\n",
    "                         labels=labels)\n",
    "    # Display precision matrix\n",
    "    plotting.plot_matrix(prec, cmap=plotting.cm.bwr,\n",
    "                         vmin=-span, vmax=span, title=\"%s / precision\" % title,\n",
    "                         labels=labels)\n",
    "\n",
    "title = \"GroupSparseCovariance\"\n",
    "plot_matrices_2(gsc.covariances_[..., 0],\n",
    "              gsc.precisions_[..., 0], title)\n",
    "\n",
    "from nilearn.plotting import find_cuts\n",
    "regions_img = regions_extracted_img\n",
    "coords_connectome = find_cuts.find_probabilistic_atlas_cut_coords(regions_img)\n",
    "\n",
    "plotting.plot_connectome(-gsc.precisions_[..., 0],\n",
    "                         coords_connectome, edge_threshold='90%',\n",
    "                         title=title,\n",
    "                         display_mode=\"lzr\",\n",
    "                         edge_vmax=.5, edge_vmin=-.5)\n",
    "\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_biomarkers = {}\n",
    "\n",
    "conn_measure = ConnectivityMeasure(kind='correlation', vectorize=True)\n",
    "connectivity_biomarkers = conn_measure.fit_transform(all_time_series)\n",
    "\n",
    "# For each kind, all individual coefficients are stacked in a unique 2D matrix.\n",
    "print('{0} correlation biomarkers for each subject.'.format(\n",
    "    connectivity_biomarkers.shape[1]))\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "classes = ['{0}{1}'.format(site_name, adhd_label)\n",
    "           for site_name, adhd_label in zip(site_names, adhd_labels)]\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mean_scores = []\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    cv_scores = cross_val_score(clf,\n",
    "                                connectivity_biomarkers,\n",
    "                                y=adhd_labels,\n",
    "                                cv=cv,\n",
    "                                groups=adhd_labels,\n",
    "                                scoring='accuracy',\n",
    "                                )\n",
    "    mean_scores.append(cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_biomarkers = {}\n",
    "\n",
    "conn_measure = ConnectivityMeasure(kind='correlation', vectorize=True)\n",
    "connectivity_biomarkers = conn_measure.fit_transform(all_time_series)\n",
    "\n",
    "# For each kind, all individual coefficients are stacked in a unique 2D matrix.\n",
    "print('{0} correlation biomarkers for each subject.'.format(\n",
    "    connectivity_biomarkers.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 3D map\n",
    "----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets     \n",
    "   \n",
    "view = plotting.view_img_on_surf(cur_img, threshold='90%', surf_mesh='fsaverage')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.open_in_browser() \n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
