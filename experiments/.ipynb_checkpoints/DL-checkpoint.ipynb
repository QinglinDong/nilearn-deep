{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Group analysis of resting-state fMRI with ICA: CanICA\n",
    "=====================================================\n",
    "\n",
    "An example applying CanICA to resting-state data. This example applies it\n",
    "to 30 subjects of the ADHD200 datasets. Then it plots a map with all the\n",
    "components together and an axial cut for each of the components separately.\n",
    "\n",
    "CanICA is an ICA method for group-level analysis of fMRI data. Compared\n",
    "to other strategies, it brings a well-controlled group model, as well as a\n",
    "thresholding algorithm controlling for specificity and sensitivity with\n",
    "an explicit model of the signal. The reference papers are:\n",
    "\n",
    "    * G. Varoquaux et al. \"A group model for stable multi-subject ICA on\n",
    "      fMRI datasets\", NeuroImage Vol 51 (2010), p. 288-299\n",
    "\n",
    "    * G. Varoquaux et al. \"ICA-based sparse features recovery from fMRI\n",
    "      datasets\", IEEE ISBI 2010, p. 1177\n",
    "\n",
    "Pre-prints for both papers are available on hal\n",
    "(http://hal.archives-ouvertes.fr)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The use of the attribute `components_img_` from decomposition\n",
    "    estimators is implemented from version 0.4.1.\n",
    "    For older versions, unmask the deprecated attribute `components_`\n",
    "    to get the components image using attribute `masker_` embedded in\n",
    "    estimator.\n",
    "    See the `section Inverse transform: unmasking data <unmasking_step>`.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 load the ADHD200 data\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First functional nifti image (4D) is at: /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uga_qinglin/Documents/nilearn-deep/nilearn/datasets/func.py:503: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype=None)\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "#dir='/raid/nilearn_data'\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=30,data_dir='/home/share/TmpData/Qinglin/nilearn_data/')\n",
    "func_filenames = adhd_dataset.func  # list of 4D nifti files for each subject\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First functional nifti image (4D) is at: %s' %\n",
    "      func_filenames[0])  # 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 apply CanICA on the data\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultiNiftiMasker.fit] Loading data from [/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz, /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.g\n",
      "[MultiNiftiMasker.fit] Computing mask\n",
      "[MultiNiftiMasker.transform] Resampling mask\n",
      "[DictLearning] Loading data\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010128/0010128_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0021019/0021019_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023008/0023008_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023012/0023012_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027011/0027011_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027018/0027018_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027034/0027034_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027037/0027037_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1019436/1019436_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1206380/1206380_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1418396/1418396_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1517058/1517058_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1552181/1552181_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1562298/1562298_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1679142/1679142_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2014113/2014113_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2497695/2497695_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2950754/2950754_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3007585/3007585_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3154996/3154996_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3205761/3205761_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3520880/3520880_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3624598/3624598_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3699991/3699991_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3884955/3884955_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3902469/3902469_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3994098/3994098_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/4016887/4016887_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[DictLearning] Learning initial components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DictLearning] Computing initial loadings\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import dict_learning_online\n",
    "from nilearn.decomposition import DictLearning, CanICA\n",
    "dict_learning = DictLearning(n_components=30,\n",
    "                             memory=\"nilearn_cache\", memory_level=2,\n",
    "                             verbose=1,\n",
    "                             random_state=0,\n",
    "                             n_epochs=1)\n",
    "data=dict_learning.prepare_data(func_filenames)\n",
    "\n",
    "dict_learning._raw_fit2(data)\n",
    "components_img=dict_learning.components_\n",
    "#_, diction=dict_learning_online(data,n_components=30,alpha=1,n_iter=100)\n",
    "#components_img=diction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 plot each ICA component separately\n",
    "-----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 64246)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'canica' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2be538ea3d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcomponents_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresholding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcomponents_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'canica' is not defined"
     ]
    }
   ],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "#masker = NiftiMasker(mask_img=canica.masker_.mask_img_, standardize=True)\n",
    "#masker.fit()\n",
    "\n",
    "\n",
    "\n",
    "components_img = dict_learning.thresholding(components_img)\n",
    "components_img = masker.inverse_transform(components_img)\n",
    "\n",
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"ortho\", title=\"IC %d\" % i,\n",
    "                  cut_coords=None, colorbar='bwr')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Extract regions from networks\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Region Extractor algorithm from regions module\n",
    "# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n",
    "# maps, less the threshold means that more intense non-voxels will be survived.\n",
    "from nilearn.regions import RegionExtractor\n",
    "\n",
    "extractor = RegionExtractor(components_img, threshold=0.5,\n",
    "                            thresholding_strategy='ratio_n_voxels',\n",
    "                            extractor='local_regions',\n",
    "                            standardize=True, min_region_size=5000)\n",
    "# Just call fit() to process for regions extraction\n",
    "extractor.fit()\n",
    "# Extracted regions are stored in regions_img_\n",
    "regions_extracted_img = extractor.regions_img_\n",
    "# Each region index is stored in index_\n",
    "regions_index = extractor.index_\n",
    "# Total number of regions extracted\n",
    "n_regions_extracted = regions_extracted_img.shape[-1]\n",
    "\n",
    "from nilearn.plotting import plot_prob_atlas\n",
    "# Visualization of region extraction results\n",
    "title = ('%d regions are extracted from %s components.'\n",
    "         '\\nEach separate color of region indicates extracted region'\n",
    "         % (n_regions_extracted, 'all'))\n",
    "plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n",
    "                         title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Region signals extraction\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to do subjects timeseries signals extraction and then estimating\n",
    "# correlation matrices on those signals.\n",
    "# To extract timeseries signals, we call transform() from RegionExtractor object\n",
    "# onto each subject functional data stored in func_filenames.\n",
    "# To estimate correlation matrices we import connectome utilities from nilearn\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "confounds = adhd_dataset.confounds\n",
    "phenotypic = adhd_dataset.phenotypic\n",
    "adhd_time_series = []\n",
    "all_time_series = []\n",
    "site_names = []\n",
    "adhd_labels = []  # 1 if ADHD, 0 if control\n",
    "# Initializing ConnectivityMeasure object with kind='correlation'\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "for filename, confound, phenotypic in zip(func_filenames, confounds,phenotypic):\n",
    "    time_series = extractor.transform(filename, confounds=confound)   \n",
    "    all_time_series.append(time_series)\n",
    "    is_adhd = phenotypic['adhd']\n",
    "    if is_adhd:\n",
    "        adhd_time_series.append(time_series)\n",
    "    site_names.append(phenotypic['site'])\n",
    "    adhd_labels.append(is_adhd)        \n",
    "print('Data has {0} ADHD subjects.'.format(len(adhd_time_series)))        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3  Correlation coefficients\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix plotting from Nilearn: nilearn.plotting.plot_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_matrices(matrices, matrix_kind):\n",
    "    n_matrices = len(matrices)\n",
    "    fig = plt.figure(figsize=(n_matrices * 4, 4))\n",
    "    for n_subject, matrix in enumerate(matrices):\n",
    "        plt.subplot(1, n_matrices, n_subject + 1)\n",
    "        matrix = matrix.copy()  # avoid side effects\n",
    "        # Set diagonal to zero, for better visualization\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        vmax = np.max(np.abs(matrix))\n",
    "        title = '{0}, subject {1}'.format(matrix_kind, n_subject)\n",
    "        plotting.plot_matrix(matrix, vmin=-vmax, vmax=vmax, cmap='RdBu_r',\n",
    "                             title=title, figure=fig, colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrices = correlation_measure.fit_transform(adhd_time_series)\n",
    "        \n",
    "from nilearn import plotting\n",
    "plot_matrices(correlation_matrices[:4], 'correlation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 group-sparse precision matrices\n",
    "------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import GroupSparseCovarianceCV\n",
    "gsc = GroupSparseCovarianceCV(verbose=2)\n",
    "gsc.fit(adhd_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrices_2(cov, prec, title, labels=None):\n",
    "    \"\"\"Plot covariance and precision matrices, for a given processing. \"\"\"\n",
    "\n",
    "    prec = prec.copy()  # avoid side effects\n",
    "\n",
    "    # Put zeros on the diagonal, for graph clarity.\n",
    "    size = prec.shape[0]\n",
    "    prec[list(range(size)), list(range(size))] = 0\n",
    "    span = max(abs(prec.min()), abs(prec.max()))\n",
    "\n",
    "    # Display covariance matrix\n",
    "    plotting.plot_matrix(cov, cmap=plotting.cm.bwr,\n",
    "                         vmin=-1, vmax=1, title=\"%s / covariance\" % title,\n",
    "                         labels=labels)\n",
    "    # Display precision matrix\n",
    "    plotting.plot_matrix(prec, cmap=plotting.cm.bwr,\n",
    "                         vmin=-span, vmax=span, title=\"%s / precision\" % title,\n",
    "                         labels=labels)\n",
    "\n",
    "title = \"GroupSparseCovariance\"\n",
    "plot_matrices_2(gsc.covariances_[..., 0],\n",
    "              gsc.precisions_[..., 0], title)\n",
    "\n",
    "from nilearn.plotting import find_cuts\n",
    "regions_img = regions_extracted_img\n",
    "coords_connectome = find_cuts.find_probabilistic_atlas_cut_coords(regions_img)\n",
    "\n",
    "plotting.plot_connectome(-gsc.precisions_[..., 0],\n",
    "                         coords_connectome, edge_threshold='90%',\n",
    "                         title=title,\n",
    "                         display_mode=\"lzr\",\n",
    "                         edge_vmax=.5, edge_vmin=-.5)\n",
    "\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import html_connectome\n",
    "view = html_connectome.view_connectome(-gsc.precisions_[..., 0], coords_connectome, threshold='95%')\n",
    "view.open_in_browser()\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 classification\n",
    "----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_biomarkers = {}\n",
    "\n",
    "conn_measure = ConnectivityMeasure(kind='correlation', vectorize=True)\n",
    "connectivity_biomarkers = conn_measure.fit_transform(all_time_series)\n",
    "\n",
    "# For each kind, all individual coefficients are stacked in a unique 2D matrix.\n",
    "print('{0} correlation biomarkers for each subject.'.format(\n",
    "    connectivity_biomarkers.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "classes = ['{0}{1}'.format(site_name, adhd_label)\n",
    "           for site_name, adhd_label in zip(site_names, adhd_labels)]\n",
    "cv = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mean_scores = []\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    cv_scores = cross_val_score(clf,\n",
    "                                connectivity_biomarkers,\n",
    "                                y=adhd_labels,\n",
    "                                cv=cv,\n",
    "                                groups=adhd_labels,\n",
    "                                scoring='accuracy',\n",
    "                                )\n",
    "    mean_scores.append(cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import show\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "positions = np.arange(len(names)) * .1 + .1\n",
    "plt.barh(positions, mean_scores, align='center', height=.05)\n",
    "yticks = [name.replace(' ', '\\n') for name in names]\n",
    "plt.yticks(positions, yticks)\n",
    "plt.xlabel('Classification accuracy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D map\n",
    "----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets     \n",
    "view = plotting.view_img_on_surf(cur_img, threshold='90%', surf_mesh='fsaverage')   \n",
    "view.open_in_browser() \n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
