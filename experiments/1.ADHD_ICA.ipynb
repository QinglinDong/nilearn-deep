{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Group analysis of resting-state fMRI with ICA: CanICA\n",
    "=====================================================\n",
    "\n",
    "An example applying CanICA to resting-state data. This example applies it\n",
    "to 30 subjects of the ADHD200 datasets. Then it plots a map with all the\n",
    "components together and an axial cut for each of the components separately.\n",
    "\n",
    "CanICA is an ICA method for group-level analysis of fMRI data. Compared\n",
    "to other strategies, it brings a well-controlled group model, as well as a\n",
    "thresholding algorithm controlling for specificity and sensitivity with\n",
    "an explicit model of the signal. The reference papers are:\n",
    "\n",
    "    * G. Varoquaux et al. \"A group model for stable multi-subject ICA on\n",
    "      fMRI datasets\", NeuroImage Vol 51 (2010), p. 288-299\n",
    "\n",
    "    * G. Varoquaux et al. \"ICA-based sparse features recovery from fMRI\n",
    "      datasets\", IEEE ISBI 2010, p. 1177\n",
    "\n",
    "Pre-prints for both papers are available on hal\n",
    "(http://hal.archives-ouvertes.fr)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The use of the attribute `components_img_` from decomposition\n",
    "    estimators is implemented from version 0.4.1.\n",
    "    For older versions, unmask the deprecated attribute `components_`\n",
    "    to get the components image using attribute `masker_` embedded in\n",
    "    estimator.\n",
    "    See the `section Inverse transform: unmasking data <unmasking_step>`.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the ADHD200 data\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First functional nifti image (4D) is at: /raid/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nilearn-deep/nilearn/datasets/func.py:502: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype=None)\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "dir='/raid/nilearn_data'\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=10,data_dir=dir)\n",
    "func_filenames = adhd_dataset.func  # list of 4D nifti files for each subject\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First functional nifti image (4D) is at: %s' %\n",
    "      func_filenames[0])  # 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply CanICA on the data\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultiNiftiMasker.fit] Loading data from [/raid/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz, /raid/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.gz, /raid/nilearn_data/adhd/data/0010128/0010\n",
      "[MultiNiftiMasker.fit] Computing mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   56.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  4.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  5.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  6.2min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "from nilearn.decomposition import CanICA\n",
    "\n",
    "canica = CanICA(n_components=20, smoothing_fwhm=6.,\n",
    "                memory=\"nilearn_cache\", memory_level=2,\n",
    "                threshold=3., verbose=10, random_state=0)\n",
    "data=canica.prepare_data(func_filenames)\n",
    "print(data.shape)\n",
    "canica.fit(data)\n",
    "# Retrieve the independent components in brain space. Directly\n",
    "# accesible through attribute `components_img_`. Note that this\n",
    "# attribute is implemented from version 0.4.1. For older versions,\n",
    "# see note section above for details.\n",
    "components_img = canica.components_img_\n",
    "# components_img is a Nifti Image object, and can be saved to a file with\n",
    "# the following line:\n",
    "components_img.to_filename('canica_resting_state.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize we plot the outline of all components on one figure\n",
    "-----------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_prob_atlas\n",
    "\n",
    "# Plot all ICA components together\n",
    "plot_prob_atlas(components_img, title='All ICA components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "plotting.plot_prob_atlas(components_img, view_type='filled_contours',\n",
    "                         title='All ICA components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the map for each ICA component separately\n",
    "-----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"z\", title=\"IC %d\" % i,\n",
    "                  cut_coords=1, colorbar=False)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extract regions from networks\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Region Extractor algorithm from regions module\n",
    "# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n",
    "# maps, less the threshold means that more intense non-voxels will be survived.\n",
    "from nilearn.regions import RegionExtractor\n",
    "\n",
    "extractor = RegionExtractor(components_img, threshold=0.5,\n",
    "                            thresholding_strategy='ratio_n_voxels',\n",
    "                            extractor='local_regions',\n",
    "                            standardize=True, min_region_size=1350)\n",
    "# Just call fit() to process for regions extraction\n",
    "extractor.fit()\n",
    "# Extracted regions are stored in regions_img_\n",
    "regions_extracted_img = extractor.regions_img_\n",
    "# Each region index is stored in index_\n",
    "regions_index = extractor.index_\n",
    "# Total number of regions extracted\n",
    "n_regions_extracted = regions_extracted_img.shape[-1]\n",
    "\n",
    "# Visualization of region extraction results\n",
    "title = ('%d regions are extracted from %d components.'\n",
    "         '\\nEach separate color of region indicates extracted region'\n",
    "         % (n_regions_extracted, 5))\n",
    "plotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n",
    "                         title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compute correlation coefficients\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to do subjects timeseries signals extraction and then estimating\n",
    "# correlation matrices on those signals.\n",
    "# To extract timeseries signals, we call transform() from RegionExtractor object\n",
    "# onto each subject functional data stored in func_filenames.\n",
    "# To estimate correlation matrices we import connectome utilities from nilearn\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "correlations = []\n",
    "# Initializing ConnectivityMeasure object with kind='correlation'\n",
    "connectome_measure = ConnectivityMeasure(kind='correlation')\n",
    "for filename, confound in zip(func_filenames, confounds):\n",
    "    # call transform from RegionExtractor object to extract timeseries signals\n",
    "    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n",
    "    # call fit_transform from ConnectivityMeasure object\n",
    "    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n",
    "    # saving each subject correlation to correlations\n",
    "    correlations.append(correlation)\n",
    "\n",
    "# Mean of all correlations\n",
    "import numpy as np\n",
    "mean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n",
    "                                                          n_regions_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot resulting connectomes\n",
    "----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Correlation between %d regions' % n_regions_extracted\n",
    "\n",
    "# First plot the matrix\n",
    "display = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n",
    "                               colorbar=True, title=title)\n",
    "\n",
    "# Then find the center of the regions and plot a connectome\n",
    "regions_img = regions_extracted_img\n",
    "coords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n",
    "\n",
    "plotting.plot_connectome(mean_correlations, coords_connectome,\n",
    "                         edge_threshold='90%', title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
