{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Group analysis of resting-state fMRI with ICA: CanICA\n",
    "=====================================================\n",
    "\n",
    "An example applying CanICA to resting-state data. This example applies it\n",
    "to 30 subjects of the ADHD200 datasets. Then it plots a map with all the\n",
    "components together and an axial cut for each of the components separately.\n",
    "\n",
    "CanICA is an ICA method for group-level analysis of fMRI data. Compared\n",
    "to other strategies, it brings a well-controlled group model, as well as a\n",
    "thresholding algorithm controlling for specificity and sensitivity with\n",
    "an explicit model of the signal. The reference papers are:\n",
    "\n",
    "    * G. Varoquaux et al. \"A group model for stable multi-subject ICA on\n",
    "      fMRI datasets\", NeuroImage Vol 51 (2010), p. 288-299\n",
    "\n",
    "    * G. Varoquaux et al. \"ICA-based sparse features recovery from fMRI\n",
    "      datasets\", IEEE ISBI 2010, p. 1177\n",
    "\n",
    "Pre-prints for both papers are available on hal\n",
    "(http://hal.archives-ouvertes.fr)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The use of the attribute `components_img_` from decomposition\n",
    "    estimators is implemented from version 0.4.1.\n",
    "    For older versions, unmask the deprecated attribute `components_`\n",
    "    to get the components image using attribute `masker_` embedded in\n",
    "    estimator.\n",
    "    See the `section Inverse transform: unmasking data <unmasking_step>`.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 load the ADHD200 data\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uga_qinglin/Documents/nilearn-deep/nilearn/datasets/func.py:503: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First functional nifti image (4D) is at: /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "#dir='/raid/nilearn_data'\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=30,data_dir='/home/share/TmpData/Qinglin/nilearn_data/')\n",
    "func_filenames = adhd_dataset.func  # list of 4D nifti files for each subject\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First functional nifti image (4D) is at: %s' %\n",
    "      func_filenames[0])  # 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 apply CanICA on the data\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultiNiftiMasker.fit] Loading data from [/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz, /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.g\n",
      "[MultiNiftiMasker.fit] Computing mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    3.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    4.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    6.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    7.5s remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "from nilearn.decomposition import CanICA\n",
    "\n",
    "canica = CanICA(n_components=20, smoothing_fwhm=6.,\n",
    "                memory=\"nilearn_cache\", memory_level=2,\n",
    "                threshold=3., verbose=10, random_state=0)\n",
    "data=canica.prepare_data(func_filenames)\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "from nilearn.decomposition.rbm import RBM\n",
    "rbm=RBM(data.shape[1],50,epochs = 10)\n",
    "\n",
    "rbm.train(data)\n",
    "# Retrieve the independent components in brain space. Directly\n",
    "# accesible through attribute `components_img_`. Note that this\n",
    "# attribute is implemented from version 0.4.1. For older versions,\n",
    "# see note section above for details.\n",
    "components_img=rbm.getW()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 plot each ICA component separately\n",
    "-----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=canica.masker_.mask_img_, standardize=True)\n",
    "masker.fit()\n",
    "\n",
    "components_img=canica.thresholding(components_img)\n",
    "components_img = masker.inverse_transform(components_img)\n",
    "\n",
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"ortho\", title=\"IC %d\" % i,\n",
    "                  cut_coords=None, colorbar='bwr')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Extract regions from networks\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Region Extractor algorithm from regions module\n",
    "# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n",
    "# maps, less the threshold means that more intense non-voxels will be survived.\n",
    "from nilearn.regions import RegionExtractor\n",
    "\n",
    "extractor = RegionExtractor(components_img, threshold=0.5,\n",
    "                            thresholding_strategy='ratio_n_voxels',\n",
    "                            extractor='local_regions',\n",
    "                            standardize=True, min_region_size=2500)\n",
    "# Just call fit() to process for regions extraction\n",
    "extractor.fit()\n",
    "# Extracted regions are stored in regions_img_\n",
    "regions_extracted_img = extractor.regions_img_\n",
    "# Each region index is stored in index_\n",
    "regions_index = extractor.index_\n",
    "# Total number of regions extracted\n",
    "n_regions_extracted = regions_extracted_img.shape[-1]\n",
    "\n",
    "from nilearn.plotting import plot_prob_atlas\n",
    "# Visualization of region extraction results\n",
    "title = ('%d regions are extracted from %s components.'\n",
    "         '\\nEach separate color of region indicates extracted region'\n",
    "         % (n_regions_extracted, 'all'))\n",
    "plot_prob_atlas(components_img, view_type='filled_contours',\n",
    "                         title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Region signals extraction\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to do subjects timeseries signals extraction and then estimating\n",
    "# correlation matrices on those signals.\n",
    "# To extract timeseries signals, we call transform() from RegionExtractor object\n",
    "# onto each subject functional data stored in func_filenames.\n",
    "# To estimate correlation matrices we import connectome utilities from nilearn\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "confounds = adhd_dataset.confounds\n",
    "phenotypic = adhd_dataset.phenotypic\n",
    "adhd_time_series = []\n",
    "# Initializing ConnectivityMeasure object with kind='correlation'\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "for filename, confound, phenotypic in zip(func_filenames, confounds,phenotypic):\n",
    "    # call transform from RegionExtractor object to extract timeseries signals\n",
    "    time_series = extractor.transform(filename, confounds=confound)    \n",
    "    is_adhd = phenotypic['adhd']\n",
    "    if is_adhd:\n",
    "        adhd_time_series.append(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3  Correlation coefficients\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix plotting from Nilearn: nilearn.plotting.plot_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_matrices(matrices, matrix_kind):\n",
    "    n_matrices = len(matrices)\n",
    "    fig = plt.figure(figsize=(n_matrices * 4, 4))\n",
    "    for n_subject, matrix in enumerate(matrices):\n",
    "        plt.subplot(1, n_matrices, n_subject + 1)\n",
    "        matrix = matrix.copy()  # avoid side effects\n",
    "        # Set diagonal to zero, for better visualization\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        vmax = np.max(np.abs(matrix))\n",
    "        title = '{0}, subject {1}'.format(matrix_kind, n_subject)\n",
    "        plotting.plot_matrix(matrix, vmin=-vmax, vmax=vmax, cmap='RdBu_r',\n",
    "                             title=title, figure=fig, colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrices = correlation_measure.fit_transform(adhd_time_series)\n",
    "        \n",
    "from nilearn import plotting\n",
    "plot_matrices(correlation_matrices[:4], 'correlation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 group-sparse precision matrices\n",
    "------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import GroupSparseCovarianceCV\n",
    "gsc = GroupSparseCovarianceCV(verbose=2)\n",
    "gsc.fit(adhd_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrices_2(cov, prec, title, labels=None):\n",
    "    \"\"\"Plot covariance and precision matrices, for a given processing. \"\"\"\n",
    "\n",
    "    prec = prec.copy()  # avoid side effects\n",
    "\n",
    "    # Put zeros on the diagonal, for graph clarity.\n",
    "    size = prec.shape[0]\n",
    "    prec[list(range(size)), list(range(size))] = 0\n",
    "    span = max(abs(prec.min()), abs(prec.max()))\n",
    "\n",
    "    # Display covariance matrix\n",
    "    plotting.plot_matrix(cov, cmap=plotting.cm.bwr,\n",
    "                         vmin=-1, vmax=1, title=\"%s / covariance\" % title,\n",
    "                         labels=labels)\n",
    "    # Display precision matrix\n",
    "    plotting.plot_matrix(prec, cmap=plotting.cm.bwr,\n",
    "                         vmin=-span, vmax=span, title=\"%s / precision\" % title,\n",
    "                         labels=labels)\n",
    "\n",
    "title = \"GroupSparseCovariance\"\n",
    "plot_matrices_2(gsc.covariances_[..., 0],\n",
    "              gsc.precisions_[..., 0], title)\n",
    "\n",
    "from nilearn.plotting import find_cuts\n",
    "regions_img = regions_extracted_img\n",
    "coords_connectome = find_cuts.find_probabilistic_atlas_cut_coords(regions_img)\n",
    "\n",
    "plotting.plot_connectome(-gsc.precisions_[..., 0],\n",
    "                         coords_connectome, edge_threshold='90%',\n",
    "                         title=title,\n",
    "                         display_mode=\"lzr\",\n",
    "                         edge_vmax=.5, edge_vmin=-.5)\n",
    "\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import html_connectome\n",
    "view = html_connectome.view_connectome(-gsc.precisions_[..., 0], coords_connectome, threshold='95%')\n",
    "view.open_in_browser()\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 3D map\n",
    "----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets     \n",
    "   \n",
    "view = plotting.view_img_on_surf(cur_img, threshold='90%', surf_mesh='fsaverage')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.open_in_browser() \n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
