{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dictionary Learning and ICA for doing group analysis of resting-state fMRI\n",
    "==========================================================================\n",
    "\n",
    "This example applies dictionary learning and ICA to resting-state data,\n",
    "visualizing resulting components using atlas plotting tools.\n",
    "\n",
    "Dictionary learning is a sparsity based decomposition method for extracting\n",
    "spatial maps. It extracts maps that are naturally sparse and usually cleaner\n",
    "than ICA\n",
    "\n",
    "   * Arthur Mensch et al. `Compressed online dictionary learning for fast resting-state fMRI decomposition\n",
    "     <https://hal.archives-ouvertes.fr/hal-01271033/>`_,\n",
    "     ISBI 2016, Lecture Notes in Computer Science\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The use of the attribute `components_img_` from decomposition\n",
    "    estimators is implemented from version 0.4.1.\n",
    "    For older versions, unmask the deprecated attribute `components_` to\n",
    "    get the components image using attribute `masker_` embedded in estimator.\n",
    "    See the `section Inverse transform: unmasking data <unmasking_step>`.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ADHD rest dataset\n",
    "-----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First functional nifti image (4D) is at: /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uga_qinglin/Documents/nilearn-deep/nilearn/datasets/func.py:503: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype=None)\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=30,data_dir='/home/share/TmpData/Qinglin/nilearn_data/')\n",
    "func_filenames = adhd_dataset.func  # list of 4D nifti files for each subject\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First functional nifti image (4D) is at: %s' %\n",
    "      adhd_dataset.func[0])  # 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two decomposition estimators\n",
    "------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decomposition import DictLearning, CanICA\n",
    "\n",
    "n_components = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary learning\n",
    "--------------------\n",
    "\n",
    "We use as \"template\" as a strategy to compute the mask, as this leads\n",
    "to slightly faster and more reproducible results. However, the images\n",
    "need to be in MNI template space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_learning = DictLearning(n_components=n_components,\n",
    "                             memory=\"nilearn_cache\", memory_level=2,\n",
    "                             verbose=1,\n",
    "                             random_state=0,\n",
    "                             n_epochs=1,\n",
    "                             mask_strategy='epi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CanICA\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "canica = CanICA(n_components=n_components,\n",
    "                memory=\"nilearn_cache\", memory_level=2,\n",
    "                threshold=3.,\n",
    "                n_init=1,\n",
    "                verbose=1,\n",
    "                mask_strategy='epi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit both estimators\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Example] Learning maps using DictionaryLearning model\n",
      "[MultiNiftiMasker.fit] Loading data from [/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz, /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.g\n",
      "[MultiNiftiMasker.fit] Computing mask\n",
      "[MultiNiftiMasker.transform] Resampling mask\n",
      "[DictLearning] Loading data\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010064/0010064_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010128/0010128_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010128/0010128_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0021019/0021019_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0021019/0021019_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023008/0023008_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023008/0023008_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023012/0023012_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0023012/0023012_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027011/0027011_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027011/0027011_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027018/0027018_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027018/0027018_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027034/0027034_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027034/0027034_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027037/0027037_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/0027037/0027037_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1019436/1019436_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1019436/1019436_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1206380/1206380_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1206380/1206380_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1418396/1418396_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1418396/1418396_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1517058/1517058_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1517058/1517058_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1552181/1552181_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1552181/1552181_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1562298/1562298_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1562298/1562298_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1679142/1679142_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/1679142/1679142_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2014113/2014113_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2014113/2014113_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2497695/2497695_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2497695/2497695_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2950754/2950754_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/2950754/2950754_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3007585/3007585_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3007585/3007585_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3154996/3154996_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3154996/3154996_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3205761/3205761_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3205761/3205761_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3520880/3520880_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3520880/3520880_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3624598/3624598_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3624598/3624598_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3699991/3699991_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3699991/3699991_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3884955/3884955_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3884955/3884955_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3902469/3902469_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3902469/3902469_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3994098/3994098_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/3994098/3994098_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "/home/share/TmpData/Qinglin/nilearn_data/adhd/data/4016887/4016887_rest_tshift_RPI_voreg_mni.nii.gz\n",
      "[MultiNiftiMasker.transform_single_imgs] Loading data from Nifti1Image('/home/share/TmpData/Qinglin/nilearn_data/adhd/data/4016887/4016887_rest_tshift_RPI_voreg_mni.nii.gz')\n",
      "[MultiNiftiMasker.transform_single_imgs] Smoothing images\n",
      "[MultiNiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[MultiNiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "[DictLearning] Learning initial components\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-791aa1a2faa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Example] Learning maps using %s model'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Example] Saving results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Grab extracted components umasked back to Nifti image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nilearn-deep/nilearn/decomposition/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nilearn-deep/nilearn/decomposition/dict_learning.py\u001b[0m in \u001b[0;36m_raw_fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[DictLearning] Learning initial components'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nilearn-deep/nilearn/decomposition/dict_learning.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# We use protected function _raw_fit as data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# has already been unmasked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mcanica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mcomponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nilearn-deep/nilearn/decomposition/canica.py\u001b[0m in \u001b[0;36m_raw_fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mcomponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unmix_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nilearn-deep/nilearn/decomposition/multi_pca.py\u001b[0m in \u001b[0;36m_raw_fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;34m\"\"\"Helper function that directly process unmasked data\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_cca\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "estimators = [dict_learning, canica]\n",
    "names = {dict_learning: 'DictionaryLearning', canica: 'CanICA'}\n",
    "components_imgs = []\n",
    "\n",
    "for estimator in estimators:\n",
    "    print('[Example] Learning maps using %s model' % names[estimator])\n",
    "    estimator.fit(func_filenames)\n",
    "    print('[Example] Saving results')\n",
    "    # Grab extracted components umasked back to Nifti image.\n",
    "    # Note: For older versions, less than 0.4.1. components_img_\n",
    "    # is not implemented. See Note section above for details.\n",
    "    components_img = estimator.components_img_\n",
    "    components_img.to_filename('%s_resting_state.nii.gz' %\n",
    "                               names[estimator])\n",
    "    components_imgs.append(components_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results\n",
    "----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import (plot_prob_atlas, find_xyz_cut_coords, show,\n",
    "                              plot_stat_map)\n",
    "from nilearn.image import index_img\n",
    "\n",
    "# Selecting specific maps to display: maps were manually chosen to be similar\n",
    "indices = {dict_learning: 25, canica: 33}\n",
    "# We select relevant cut coordinates for displaying\n",
    "cut_component = index_img(components_imgs[0], indices[dict_learning])\n",
    "cut_coords = find_xyz_cut_coords(cut_component)\n",
    "for estimator, components in zip(estimators, components_imgs):\n",
    "    # 4D plotting\n",
    "    plot_prob_atlas(components, view_type=\"filled_contours\",\n",
    "                    title=\"%s\" % names[estimator],\n",
    "                    cut_coords=cut_coords, colorbar=False)\n",
    "    # 3D plotting\n",
    "    plot_stat_map(index_img(components, indices[estimator]),\n",
    "                  title=\"%s\" % names[estimator],\n",
    "                  cut_coords=cut_coords, colorbar=False)\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
